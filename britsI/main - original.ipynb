{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from brits2_i_original.ipynb\n",
      "importing Jupyter notebook from rits2_i_original.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random \n",
    "import torch as T\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, IterableDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import argparse\n",
    "import import_ipynb\n",
    "from brits2_i_original import *\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "save_path = \"data/saved_models/activityReverse.tar\"#vaegan_model - Copy.tar\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "if not os.path.exists(\"data/saved_models\"):\n",
    "    os.makedirs(\"data/saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--eval'], dest='eval', nargs=None, const=None, default=True, type=None, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARG_PARSER = ArgumentParser()\n",
    "\n",
    "ARG_PARSER.add_argument('--nfeatures', default=609, type=int)\n",
    "ARG_PARSER.add_argument('--dfeatures', default=43, type=int)\n",
    "ARG_PARSER.add_argument('--ehidden', default=300, type=int)\n",
    "ARG_PARSER.add_argument('--model', type=str)\n",
    "\n",
    "ARG_PARSER.add_argument('--ehr', default=True)\n",
    "\n",
    "ARG_PARSER.add_argument('--num_epochs', default=100, type=int)\n",
    "ARG_PARSER.add_argument('--seq_len', default=20, type=int)\n",
    "ARG_PARSER.add_argument('--pred_len', default=8, type=int)\n",
    "ARG_PARSER.add_argument('--missingRate', default=10, type=int)\n",
    "ARG_PARSER.add_argument('--patience', default=100, type=int)\n",
    "ARG_PARSER.add_argument('--e_lrn_rate', default=0.1, type=float)\n",
    "ARG_PARSER.add_argument('--g_lrn_rate', default=0.1, type=float)\n",
    "ARG_PARSER.add_argument('--d_lrn_rate', default=0.001, type=float)\n",
    "ARG_PARSER.add_argument('--resume_training', default=False)\n",
    "ARG_PARSER.add_argument('--train', default=False)\n",
    "ARG_PARSER.add_argument('--evalImp', default=True)\n",
    "ARG_PARSER.add_argument('--evalPred', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS = ARG_PARSER.parse_args(args=[])\n",
    "MAX_SEQ_LEN = ARGS.seq_len\n",
    "BATCH_SIZE = ARGS.batch_size\n",
    "EPSILON = 1e-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, path, chunksize,length,seq_len,flag):\n",
    "        self.path = path\n",
    "        self.chunksize = chunksize\n",
    "        self.len = int(length)#number of times total getitem is called\n",
    "        self.seq_len=seq_len\n",
    "        self.flag=flag\n",
    "        self.reader=pd.read_csv(\n",
    "                self.path,header=0,\n",
    "                chunksize=self.chunksize)#,names=['data']))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.reader.get_chunk(self.chunksize)\n",
    "#         sex=pd.read_csv('C:\\\\Users/mehak/Desktop/demo.csv',header=0)\n",
    "#         sex=sex[['person_id','Sex']]\n",
    "#         data = pd.merge(data, sex, how='left', on=['person_id'])\n",
    "        #data=data[data['Age']<=10]\n",
    "        data=data.sort_values(by=['person_id','Age'])\n",
    "        #print(data['RANDOM_PATIENT_ID'].unique())\n",
    "        del data['person_id']\n",
    "        del data['visit_start_datetime']\n",
    "        del data['visit_end_datetime']\n",
    "        del data['val']\n",
    "        del data['Interval']\n",
    "        #print(data.shape)\n",
    "        #print(data.columns)\n",
    "        data.rename({'MEAS_3038553': 'BMI'}, axis=1, inplace=True)\n",
    "        \n",
    "        if(self.flag==0):\n",
    "            data=data.replace(np.inf,-1)\n",
    "            data=data.replace(-1,np.nan)\n",
    "            \n",
    "            #print(self.flag)\n",
    "            cols_to_norm = [col for col in data if (col.startswith('MEAS_') or col.startswith('C_MEAS_') or col.startswith('Weight'))]\n",
    "            data_norm=StandardScaler()\n",
    "            #bmi_scaler.fit(bmi)\n",
    "            data[cols_to_norm]=data_norm.fit_transform(data[cols_to_norm])\n",
    "\n",
    "            \n",
    "        \n",
    "            data=data.replace(np.nan,0)\n",
    "\n",
    "           # data = data.to_numpy()\n",
    "            #data = np.reshape(data, (int(data.shape[0]/99), 99, data.shape[1]))\n",
    "            data = T.as_tensor(data.values.astype(float), dtype=T.float32)\n",
    "            #data=T.from_numpy(data)\n",
    "            #data=data.double()\n",
    "            data=data.view(int(data.shape[0]/self.seq_len), self.seq_len, data.shape[1])\n",
    "            return data\n",
    "        else:\n",
    "            #print(self.flag)\n",
    "            data=data.replace(np.nan,0)\n",
    "            data = T.as_tensor(data.values.astype(float), dtype=T.float32)\n",
    "            data=data.view(int(data.shape[0]/self.seq_len), self.seq_len, data.shape[1])\n",
    "            return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf#11.1179\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, optimizer, save_path):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, optimizer, save_path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, optimizer, save_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, optimizer, save_path):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        T.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            'trainer': optimizer.state_dict()\n",
    "        }, save_path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pred_test(args, model,predWin):\n",
    "    model.eval()\n",
    "    \n",
    "    RLoss=0\n",
    "    FLoss=0\n",
    "    mseLoss=0\n",
    "    mseLossF=0\n",
    "    TBatches=0\n",
    "    oBmi=[]\n",
    "    oBmiF=[]\n",
    "    iBmi=[]\n",
    "    oAge=[]\n",
    "    oSex=[]\n",
    "    imputations=[]\n",
    "    with T.autograd.no_grad():\n",
    "        files = '.../aaai/data/ehr/preprocess/test/finalTest1054.csv'\n",
    "    \n",
    "        maskFiles = '.../aaai/data/ehr/preprocess/test/mask/maskTest1054.csv'\n",
    "        \n",
    "        dataset = CSVDataset(files, int(args.seq_len*500),1356100,args.seq_len,flag=0)\n",
    "        maskDataset = CSVDataset(maskFiles, int(args.seq_len*500),1356100, args.seq_len,flag=1)\n",
    "\n",
    "        loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)#number of times getitem is called in one iteration\n",
    "        maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "        \n",
    "        loss={}\n",
    "\n",
    "        #for every batch\n",
    "        for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "            #bmi_norm=dataset.bmi_norm\n",
    "            #print('batch: {}'.format(batch_idx))\n",
    "            data,mask=allData\n",
    "            decay=mask[:,:,:,608]\n",
    "            rdecay=mask[:,:,:,609]\n",
    "            mask=mask[:,:,:,316]\n",
    "\n",
    "            data=data.squeeze()\n",
    "            mask=mask.squeeze()\n",
    "            decay=decay.squeeze()\n",
    "            rdecay=rdecay.squeeze()\n",
    "            #print(data.shape)\n",
    "            #print(mask.shape)\n",
    "            #print(decay.shape)\n",
    "            \n",
    "            #values to be predicted\n",
    "            y = data.clone().detach()\n",
    "            testMask = mask.clone().detach()\n",
    "#             sex=y[:,:,608]\n",
    "#             age=y[:,:,607]\n",
    "            #print(sex.shape,age.shape)\n",
    "            y=y[:,:,316]\n",
    "            #print(y[0])\n",
    "            data=data[:,:,0:608]\n",
    "            \n",
    "            #------------remove last 5 timestamps------------------\n",
    "            #print(data[0:10,8:,653])\n",
    "            for i in range(data.shape[0]):\n",
    "                #if(data[i,])\n",
    "                j=20\n",
    "                if(predWin==8):\n",
    "                    k=16\n",
    "                    decay[i,j-k:j]=T.tensor([0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6,6.5,7,7.5,8])\n",
    "                    rdecay[i,j-k:j]=T.tensor([8,7.5,7,6.5,6,5.5,5,4.5,4,3.5,3,2.5,2,1.5,1,0.5])\n",
    "                elif(predWin==7):\n",
    "                    k=14\n",
    "                    decay[i,j-k:j]=T.tensor([0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6,6.5,7])\n",
    "                    rdecay[i,j-k:j]=T.tensor([7,6.5,6,5.5,5,4.5,4,3.5,3,2.5,2,1.5,1,0.5])*120\n",
    "                elif(predWin==6):\n",
    "                    k=12\n",
    "                    decay[i,j-k:j]=T.tensor([0.5,1,1.5,2,2.5,3,3.5,4,4.5,5,5.5,6])\n",
    "                    rdecay[i,j-k:j]=T.tensor([6,5.5,5,4.5,4,3.5,3,2.5,2,1.5,1,0.5])\n",
    "                elif(predWin==5):\n",
    "                    k=10\n",
    "                    decay[i,j-k:j]=T.tensor([0.5,1,1.5,2,2.5,3,3.5,4,4.5,5])\n",
    "                    rdecay[i,j-k:j]=T.tensor([5,4.5,4,3.5,3,2.5,2,1.5,1,0.5])\n",
    "                \n",
    "                data[i,j-k:j,:]=0\n",
    "                mask[i,j-k:j]=0\n",
    "                y[i,0:j-k]=0\n",
    "#                 age[i,0:j-k]=0\n",
    "#                 sex[i,0:j-k]=0\n",
    "                #print(sex.shape,age.shape)\n",
    "                #yOrig[i,0:j-k]=0\n",
    "                testMask[i,0:j-k]=0\n",
    "\n",
    "\n",
    "            ret_f, ret = run_on_batch(model,data,mask,decay,rdecay,args, optimizer=None)#,bmi_norm)\n",
    "#             print(\"Input\",data.shape)\n",
    "#             print(data[0,:,316])\n",
    "#             print(\"Reverse\",ret['imputations'][0,:])\n",
    "#             print(\"ForwardOnly\",ret_f['imputations'][0,:])\n",
    "#             print(\"Original\",y.shape)\n",
    "#             print(y[0,:])\n",
    "#             print(\"Mask\",testMask.shape)\n",
    "#             print(testMask[0,:])\n",
    "            RLoss=RLoss+ret['loss']\n",
    "            FLoss=FLoss+ret_f['loss']\n",
    "            testMask=testMask.cuda()\n",
    "            y=y.cuda()\n",
    "            outputBMI=ret['imputations'] * testMask\n",
    "            outputBMIF=ret_f['imputations'] * testMask\n",
    "            mseLoss=mseLoss+ (torch.sum(torch.abs(outputBMI-y)))/ (torch.sum(testMask) + 1e-5)\n",
    "            mseLossF=mseLossF+ (torch.sum(torch.abs(outputBMIF-y)))/ (torch.sum(testMask) + 1e-5)\n",
    "#             print(\"RMSELoss Revrese: \",mseLoss)\n",
    "#             print(\"RMSELoss Forward: \",mseLossF)\n",
    "            outBmi, outBmiF,inBmi = plotBmi(outputBMI, outputBMIF, y, testMask)\n",
    "            oBmi.extend(outBmi)\n",
    "            oBmiF.extend(outBmiF)\n",
    "            iBmi.extend(inBmi)\n",
    "\n",
    "            #T.cuda.empty_cache()\n",
    "            #paramsE=list(model['e'].parameters())\n",
    "            #paramsG=list(model['g'].parameters())\n",
    "            #print(\"AFTER PARAM\",paramsE[0][20],paramsG[8][0][0])  \n",
    "        TBatches=TBatches+batch_idx+1\n",
    "        RLoss = RLoss/TBatches\n",
    "        mseLoss = mseLoss/TBatches\n",
    "        mseLossF = mseLossF/TBatches\n",
    "    #print(\"===================================\")\n",
    "    oBmi=np.asarray(oBmi)\n",
    "    iBmi=np.asarray(iBmi)\n",
    "    loss = (oBmi - iBmi)\n",
    "    loss=np.asarray([abs(number) for number in loss])\n",
    "    variance = sum([((x - mseLoss) ** 2) for x in loss]) / len(loss) \n",
    "    res = variance ** 0.5\n",
    "    ci=1.96*(res/(math.sqrt(len(loss))))\n",
    "\n",
    "    #print(\"Val R Loss:\",RLoss)\n",
    "    print(\"CI\",ci)\n",
    "    print(\"MAE Loss Reverse:\",mseLoss)\n",
    "    #print(\"MAE Loss Forward:\",mseLossF)\n",
    "    #print(outputBMI)\n",
    "    return oBmi,oBmiF,iBmi\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation_test(args, model,missingRate):\n",
    "    model.eval()\n",
    "    \n",
    "    RLoss=0\n",
    "    FLoss=0\n",
    "    mseLoss=0\n",
    "    mseLossF=0\n",
    "    TBatches=0\n",
    "    oBmi=[]\n",
    "    oBmiF=[]\n",
    "    iBmi=[]\n",
    "    oAge=[]\n",
    "    oSex=[]\n",
    "    imputations=[]\n",
    "    samples=0\n",
    "    pids=0\n",
    "    with T.autograd.no_grad():\n",
    "        files = '.../aaai/data/ehr/preprocess/test/finalTest1054.csv'\n",
    "    \n",
    "        maskFiles = '.../aaai/data/ehr/preprocess/test/mask/maskTest1054.csv'\n",
    "        \n",
    "        dataset = CSVDataset(files, int(args.seq_len*500),1356100,args.seq_len,flag=0)\n",
    "        maskDataset = CSVDataset(maskFiles, int(args.seq_len*500),1356100, args.seq_len,flag=1)\n",
    "\n",
    "        loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)#number of times getitem is called in one iteration\n",
    "        maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "        \n",
    "        loss={}\n",
    "\n",
    "        #for every batch\n",
    "        for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "            #bmi_norm=dataset.bmi_norm\n",
    "            #print('batch: {}'.format(batch_idx))\n",
    "            data,mask=allData\n",
    "            decay=mask[:,:,:,608]\n",
    "            rdecay=mask[:,:,:,609]\n",
    "            mask=mask[:,:,:,316]\n",
    "\n",
    "            data=data.squeeze()\n",
    "            mask=mask.squeeze()\n",
    "            decay=decay.squeeze()\n",
    "            rdecay=rdecay.squeeze()\n",
    "            #print(data.shape)\n",
    "            #print(mask.shape)\n",
    "            #print(decay.shape)\n",
    "            \n",
    "            #values to be predicted\n",
    "            y = data.clone().detach()\n",
    "            testMask = mask.clone().detach()\n",
    "#             sex=y[:,:,608]\n",
    "#             age=y[:,:,607]\n",
    "            #print(sex.shape,age.shape)\n",
    "            y=y[:,:,316]\n",
    "            #print(y[0])\n",
    "            data=data[:,:,0:608]\n",
    "            \n",
    "            #------------remove last 5 timestamps------------------\n",
    "            #print(data[0:10,8:,653])\n",
    "            for i in range(data.shape[0]):\n",
    "                #if(data[i,])\n",
    "                j=20\n",
    "                k=16\n",
    "                #mask[i,:].loc[mask[i,:].query('value == 1').sample(frac=.1).index,'value'] = 0\n",
    "                idxs = torch.nonzero(mask[i,:] == 1)\n",
    "                samples=samples+list(idxs.size())[0]\n",
    "                if(missingRate==50):\n",
    "                    if(list(idxs.size())[0]>4):\n",
    "                        idxs=random.sample(set(idxs),5)\n",
    "                        data[i,idxs[0],316]=0\n",
    "                        data[i,idxs[1],316]=0\n",
    "                        data[i,idxs[2],316]=0\n",
    "                        data[i,idxs[3],316]=0\n",
    "                        data[i,idxs[4],316]=0\n",
    "                        #print(mask[i,:])\n",
    "                        mask[i,idxs[0]]=0\n",
    "                        mask[i,idxs[1]]=0\n",
    "                        mask[i,idxs[2]]=0\n",
    "                        mask[i,idxs[3]]=0\n",
    "                        mask[i,idxs[4]]=0\n",
    "                        pids=pids + 5\n",
    "                        break;\n",
    "                if(missingRate>=40):\n",
    "                    if(list(idxs.size())[0]>3):\n",
    "                        idxs=random.sample(set(idxs),4)\n",
    "                        data[i,idxs[0],316]=0\n",
    "                        data[i,idxs[1],316]=0\n",
    "                        data[i,idxs[2],316]=0\n",
    "                        data[i,idxs[3],316]=0\n",
    "                        #print(mask[i,:])\n",
    "                        mask[i,idxs[0]]=0\n",
    "                        mask[i,idxs[1]]=0\n",
    "                        mask[i,idxs[2]]=0\n",
    "                        mask[i,idxs[3]]=0\n",
    "                        pids=pids + 4\n",
    "                        break;\n",
    "                if(missingRate>=30):\n",
    "                    if(list(idxs.size())[0]>2):\n",
    "                        idxs=random.sample(set(idxs),3)\n",
    "                        data[i,idxs[0],316]=0\n",
    "                        data[i,idxs[1],316]=0\n",
    "                        data[i,idxs[2],316]=0\n",
    "                        #print(mask[i,:])\n",
    "                        mask[i,idxs[0]]=0\n",
    "                        mask[i,idxs[1]]=0\n",
    "                        mask[i,idxs[2]]=0\n",
    "                        pids=pids + 3\n",
    "                        break;\n",
    "                if(missingRate>=20):\n",
    "                    if(list(idxs.size())[0]>1):\n",
    "                        idxs=random.sample(set(idxs),2)\n",
    "                        data[i,idxs[0],316]=0\n",
    "                        data[i,idxs[1],316]=0\n",
    "                        #print(mask[i,:])\n",
    "                        mask[i,idxs[0]]=0\n",
    "                        mask[i,idxs[1]]=0\n",
    "                        pids=pids + 2\n",
    "                        break;\n",
    "                if(missingRate>=10):\n",
    "                    if(list(idxs.size())[0]>0):\n",
    "                        idxs=random.sample(set(idxs),1)\n",
    "                        data[i,idxs,316]=0\n",
    "                        mask[i,idxs]=0\n",
    "                        pids=pids + 1\n",
    "\n",
    "                #remove values from last 50% of the sequence\n",
    "                \n",
    "                #print(mask[i,:])\n",
    "                testMask[i,:]=testMask[i,:]-mask[i,:]\n",
    "                #print(testMask[i,:])\n",
    "                #print(y[i,:])\n",
    "                y[i,:]=y[i,:]*testMask[i,:]\n",
    "#                 age[i,:]=age[i,:]*testMask[i,:]\n",
    "#                 sex[i,:]=sex[i,:]*testMask[i,:]\n",
    "                \n",
    "\n",
    "            ret_f,ret = run_on_batch(model,data,mask,decay,rdecay,args, optimizer=None)#,bmi_norm)\n",
    "#             print(\"Input\",data.shape)\n",
    "#             print(data[0,:,316])\n",
    "#             print(\"Reverse\",ret['imputations'][0,:])\n",
    "#             print(\"ForwardOnly\",ret_f['imputations'][0,:])\n",
    "#             print(\"Original\",y.shape)\n",
    "#             print(y[0,:])\n",
    "#             print(\"Mask\",testMask.shape)\n",
    "#             print(testMask[0,:])\n",
    "            RLoss=RLoss+ret['loss']\n",
    "            FLoss=FLoss+ret_f['loss']\n",
    "            testMask=testMask.cuda()\n",
    "            y=y.cuda()\n",
    "            outputBMI=ret['imputations'] * testMask\n",
    "            outputBMIF=ret_f['imputations'] * testMask\n",
    "            mseLoss=mseLoss+ (torch.sum(torch.abs(outputBMI-y)))/ (torch.sum(testMask) + 1e-5)\n",
    "            mseLossF=mseLossF+ (torch.sum(torch.abs(outputBMIF-y)))/ (torch.sum(testMask) + 1e-5)\n",
    "#             print(\"RMSELoss Revrese: \",mseLoss)\n",
    "#             print(\"RMSELoss Forward: \",mseLossF)\n",
    "            outBmi, outBmiF,inBmi = plotBmi(outputBMI, outputBMIF, y, testMask)\n",
    "            oBmi.extend(outBmi)\n",
    "            oBmiF.extend(outBmiF)\n",
    "            iBmi.extend(inBmi)\n",
    "\n",
    "        TBatches=TBatches+batch_idx+1\n",
    "        RLoss = RLoss/TBatches\n",
    "        mseLoss = mseLoss/TBatches\n",
    "        mseLossF = mseLossF/TBatches\n",
    "    #print(\"===================================\")\n",
    "    oBmi=np.asarray(oBmi)\n",
    "    iBmi=np.asarray(iBmi)\n",
    "    loss = (oBmi - iBmi)\n",
    "    loss=np.asarray([abs(number) for number in loss])\n",
    "    variance = sum([((x - mseLoss) ** 2) for x in loss]) / len(loss) \n",
    "    res = variance ** 0.5\n",
    "    ci=1.96*(res/(math.sqrt(len(loss))))\n",
    "\n",
    "    #print(\"Val R Loss:\",RLoss)\n",
    "    print(\"CI\",ci)\n",
    "    print(\"MAE Loss Reverse:\",mseLoss)\n",
    "    #print(\"MAE Loss Forward:\",mseLossF)\n",
    "    #print(outputBMI)\n",
    "    return oBmi,oBmiF,iBmi\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBmi(outBmi,outBmiF, inBmi, testMask):\n",
    "    \n",
    "    outBmi = outBmi.cpu().detach().numpy()\n",
    "    outBmiF = outBmiF.cpu().detach().numpy()\n",
    "    inBmi = inBmi.cpu().detach().numpy()\n",
    "    testMask = testMask.cpu().detach().numpy()\n",
    "    \n",
    "    #import matplotlib.pyplot as plt\n",
    "    #%matplotlib inline\n",
    "    #from matplotlib.ticker import MultipleLocator\n",
    "    outBmi=outBmi[np.nonzero(testMask)]\n",
    "    outBmiF=outBmiF[np.nonzero(testMask)]\n",
    "    inBmi=inBmi[np.nonzero(testMask)]\n",
    "    \n",
    "    #print(outBmi)\n",
    "    #print(inBmi)\n",
    "    #print(outAge)\n",
    "    #print(outSex)\n",
    "    \n",
    "    #print(sex.shape,age.shape)\n",
    "    \n",
    "    return outBmi,outBmiF,inBmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evalFull(args, model):\n",
    "    model.eval()\n",
    "    \n",
    "    RLoss=0\n",
    "    TBatches=0\n",
    "    oBmi=[]\n",
    "    iBmi=[]\n",
    "    oAge=[]\n",
    "    oSex=[]\n",
    "   \n",
    "    with T.autograd.no_grad():\n",
    "        files = '.../aaai/data/ehr/preprocess/val/finalVal1054.csv'\n",
    "    \n",
    "        maskFiles = '.../aaai/data/ehr/preprocess/val/mask/maskVal1054.csv'\n",
    "        #print(files)\n",
    "        \n",
    "        dataset = CSVDataset(files, int(args.seq_len*500),1356100,args.seq_len,flag=0)\n",
    "        #orig = CSVDataset('C:\\\\Users/mehak/Desktop/testganAggOrig.csv', int(args.seq_len*500),1356100,args.seq_len)\n",
    "        maskDataset = CSVDataset(maskFiles, int(args.seq_len*500),1356100, args.seq_len,flag=1)\n",
    "\n",
    "        loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)#number of times getitem is called in one iteration\n",
    "        #origLoader = DataLoader(orig,batch_size=1,num_workers=0, shuffle=False)\n",
    "        maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "        \n",
    "        loss={}\n",
    "\n",
    "        #for every batch\n",
    "        for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "            #bmi_norm=dataset.bmi_norm\n",
    "            #print('batch: {}'.format(batch_idx))\n",
    "            data,mask=allData\n",
    "            decay=mask[:,:,:,608]\n",
    "            rdecay=mask[:,:,:,609]\n",
    "            mask=mask[:,:,:,316]\n",
    "\n",
    "            data=data.squeeze()\n",
    "            mask=mask.squeeze()\n",
    "            decay=decay.squeeze()\n",
    "            rdecay=rdecay.squeeze()\n",
    "            #print(data.shape)\n",
    "            #print(mask.shape)\n",
    "            #print(decay.shape)\n",
    "\n",
    "\n",
    "            ret_f, ret = run_on_batch(model,data,mask,decay,rdecay, args, optimizer=None)#,bmi_norm)\n",
    "            RLoss=RLoss+ret['loss']\n",
    "\n",
    "            #T.cuda.empty_cache()\n",
    "            #paramsE=list(model['e'].parameters())\n",
    "            #paramsG=list(model['g'].parameters())\n",
    "            #print(\"AFTER PARAM\",paramsE[0][20],paramsG[8][0][0])  \n",
    "        TBatches=TBatches+batch_idx+1\n",
    "        RLoss = RLoss/TBatches\n",
    "    #print(\"===================================\")\n",
    "    print(\"Val R Loss:\",RLoss)\n",
    "    #print(outputBMI)\n",
    "    return RLoss\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(args, model):\n",
    "    ''' Run a single epoch\n",
    "    '''\n",
    "\n",
    "    trainLoss=[]\n",
    "    valLoss=[]\n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=args.patience, verbose=True)\n",
    "    if args.resume_training:\n",
    "        early_stopping(133, model, optimizer, save_path)\n",
    "        \n",
    "    #define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    \n",
    "    #for evrey epoch\n",
    "    for epoch in range(args.num_epochs):\n",
    "        model.train()\n",
    "    \n",
    "        #Running Losses\n",
    "        RLoss=0\n",
    "        TBatches=0  \n",
    "        print(\"=============EPOCH=================\")\n",
    "       \n",
    "        path = '.../aaai/data/ehr/preprocess/train/'\n",
    "        files = list(map(lambda x : path + x, (filter(lambda x : x.endswith(\"csv\"), os.listdir(path)))))\n",
    "        \n",
    "        maskPath = '.../aaai/data/ehr/preprocess/train/mask/'\n",
    "        maskFiles = list(map(lambda x : maskPath + x, (filter(lambda x : x.endswith(\"csv\"), os.listdir(maskPath)))))\n",
    "        \n",
    "        #print(files)\n",
    "        #print(maskFiles)\n",
    "        #print(\"====================New File========================\")\n",
    "        dataset = CSVDataset(files[0], int(args.seq_len*500),1356100,args.seq_len,flag=0)\n",
    "        maskDataset = CSVDataset(maskFiles[0], int(args.seq_len*500),1356100, args.seq_len,flag=1)\n",
    "\n",
    "        loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)#number of times getitem is called in one iteration\n",
    "        maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "        #for every batch\n",
    "        for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "            #bmi_norm=dataset.bmi_norm\n",
    "            #print('batch: {}'.format(batch_idx))\n",
    "            data,mask=allData\n",
    "            decay=mask[:,:,:,608]\n",
    "            rdecay=mask[:,:,:,609]\n",
    "            mask=mask[:,:,:,316]\n",
    "\n",
    "            data=data.squeeze()\n",
    "            mask=mask.squeeze()\n",
    "            decay=decay.squeeze()\n",
    "            rdecay=rdecay.squeeze()\n",
    "            #print(data.shape)\n",
    "            #print(mask.shape)\n",
    "            #print(decay.shape)\n",
    "\n",
    "\n",
    "            ret_f, ret = run_on_batch(model,data,mask,decay,rdecay, args, optimizer)#,bmi_norm)\n",
    "            RLoss=RLoss+ret['loss'].item()\n",
    "\n",
    "            #T.cuda.empty_cache()\n",
    "            #paramsE=list(model['e'].parameters())\n",
    "            #paramsG=list(model['g'].parameters())\n",
    "            #print(\"AFTER PARAM\",paramsE[0][20],paramsG[8][0][0])   \n",
    "\n",
    "        TBatches=TBatches+batch_idx+1\n",
    "        #print(TBatches)\n",
    "        #print(\"File:\", i, \"loss_R:\", \"%.4f\"%RLoss/(batch_idx+1), \"loss_G:\", \"%.4f\"%GLoss/(batch_idx+1), \"loss_D:\", \"%.4f\"%DLoss/(batch_idx+1))\n",
    "            #print(len(encoded))\n",
    "        RLoss=RLoss/TBatches\n",
    "        \n",
    "        print(\"EPOCH:\", epoch, \"loss_R:\", \"%.4f\"%RLoss)\n",
    "        \n",
    "        trainLoss.append(RLoss)\n",
    "\n",
    "        valid_loss = run_evalFull(args, model)\n",
    "        \n",
    "        valLoss.append(valid_loss)\n",
    "        #plotBmi(outBmi , inBmi)\n",
    "\n",
    "        \n",
    "        #if epoch<1 or epoch >5:\n",
    "        if not (T.isnan(valid_loss)):\n",
    "            early_stopping(valid_loss, model, optimizer, save_path)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        #plot_grad_flow(model['e'].named_parameters())\n",
    "        #plot_grad_flow(model['g'].named_parameters())\n",
    "        #plot_grad_flow(model['d'].named_parameters())\n",
    "    return trainLoss, valLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    \n",
    "    train_on_gpu = T.cuda.is_available()\n",
    "    if train_on_gpu:\n",
    "        print('Training on GPU.')\n",
    "    else:\n",
    "        print('No GPU available, training on CPU.')\n",
    "        \n",
    "    model = BRITS2()\n",
    "    \n",
    "    #print(\"Model\",model)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    if args.resume_training:\n",
    "        checkpoint = T.load(save_path)\n",
    "        model.load_state_dict(checkpoint)\n",
    "        optimizer.load_state_dict(checkpoint)\n",
    "        output = run_epoch(args, model) \n",
    "        #return model,output\n",
    "    \n",
    "    elif args.train:\n",
    "        trainLoss, valLoss = run_epoch(args, model) \n",
    "        #return trainLoss, valLoss\n",
    "        \n",
    "    elif args.evalImp:\n",
    "        #load Model\n",
    "        checkpoint = T.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "        optimizer.load_state_dict(checkpoint['trainer'])\n",
    "        oBmi, oBmiF, iBmi = imputation_test(args, model,args.missingRate)\n",
    "        \n",
    "    elif args.evalPred:\n",
    "        #load Model\n",
    "        checkpoint = T.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "        optimizer.load_state_dict(checkpoint['trainer'])\n",
    "        oBmi, oBmiF, iBmi = pred_test(args, model,args.pred_len)\n",
    "        \n",
    "        #return oBmi, oBmiF, iBmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n",
      "MSE Loss Reverse: tensor(2.8642, device='cuda:0')\n",
      "Missing%:  0.21005759069049457\n"
     ]
    }
   ],
   "source": [
    "#trainLoss, valLoss = run(ARGS)\n",
    "#oBmi, oBmiF, iBmi, oAge, oSex = run(ARGS)\n",
    "run(ARGS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Atena] *",
   "language": "python",
   "name": "conda-env-Atena-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
