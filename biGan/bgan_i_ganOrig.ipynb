{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ugan_i_ganOrig.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "import import_ipynb\n",
    "from ugan_i_ganOrig import *\n",
    "from sklearn import metrics\n",
    "\n",
    "SEQ_LEN = 36\n",
    "RNN_HID_SIZE = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGAN(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(BGAN, self).__init__()\n",
    "        self.build(args)\n",
    "        \n",
    "\n",
    "    def build(self,args):\n",
    "        self.ugan_f = UGAN(args)\n",
    "        self.ugan_b = UGAN(args)\n",
    "\n",
    "    def forward(self, data, mask, decay,rdecay,args):\n",
    "        ret_f = self.ugan_f(data, mask, decay, args, 'forward')\n",
    "        \n",
    "        #print(\"=====================================REVERSE===================================================\")\n",
    "        ret_b = self.reverse(self.ugan_b(data, mask, rdecay, args, 'backward'))\n",
    "\n",
    "        #print(\"going to merge results\")\n",
    "        ret = self.merge_ret(ret_f, ret_b)\n",
    "\n",
    "        return ret_f,ret\n",
    "\n",
    "    def merge_ret(self, ret_f, ret_b):\n",
    "        loss_f = ret_f['loss']\n",
    "        loss_b = ret_b['loss']\n",
    "        loss_c = self.get_consistency_loss(ret_f['imputations'], ret_b['imputations'])\n",
    "        \n",
    "        #print(loss_f,loss_b,loss_c)\n",
    "        #print(\"Foward Imputation\",ret_f['imputations'][0,:])\n",
    "        #print(\"Backward Imputation\",ret_b['imputations'][0,:])\n",
    "        #print(\"Imputations\",ret_f['imputations'][0,:])\n",
    "        #print(\"Fwd Combinations\",ret_f['combinations'][0,:])\n",
    "        #print(\"Missing\",ret_f['missing'][0,:])\n",
    "        #print(\"Originals\",ret_f['originals'][0,:])\n",
    "        \n",
    "        #print(\"Imputations\",ret_b['imputations'][0,:])\n",
    "        #print(\"Bwd Combinations\",ret_b['combinations'][0,:])\n",
    "        #print(\"Missing\",ret_b['missing'][0,:])\n",
    "        #print(\"Originals\",ret_b['originals'][0,:])\n",
    "\n",
    "        #loss = loss_f + loss_b + loss_c\n",
    "\n",
    "        imputations = ret_f['imputations'] * ret_f['combinations'] + ret_b['imputations'] * ret_b['combinations']\n",
    "        ret_b['imputations'] = imputations\n",
    "        #print(\"Final Imputations\",ret_b['imputations'][0,:])\n",
    "        #ret_b['originals'] = ret_b['originals'] * ret_b['missing']\n",
    "        #imputations = (ret_f['imputations'] + ret_b['imputations']) / 2\n",
    "        \n",
    "        x_loss = torch.sum(torch.abs(ret_b['originals'] - ret_b['imputations']) * ret_b['missing']) / (torch.sum(ret_b['missing']) + 1e-5)\n",
    "\n",
    "        ret_b['originals'] = ret_b['originals'] * ret_b['missing'] + ret_b['imputations'] * (1-ret_b['missing'])\n",
    "        #print(\"Complement Vector\",ret_b['originals'][0,:])\n",
    "        #ret_b['loss'] = loss\n",
    "        ret_b['loss'] = x_loss+loss_c\n",
    "        #print(\"loss\",ret_b['loss'])\n",
    "        #ret_f['predictions'] = predictions\n",
    "        #ret_b['imputations'] = imputations\n",
    "\n",
    "        return ret_b\n",
    "\n",
    "    def get_consistency_loss(self, pred_f, pred_b):\n",
    "        loss = torch.pow(pred_f - pred_b, 2.0).mean()\n",
    "        return loss\n",
    "\n",
    "    def reverse(self, ret):\n",
    "        #print(\"in Reverse\")\n",
    "        def reverse_tensor(tensor_):\n",
    "            if tensor_.dim() <= 1:\n",
    "                #print(\"dim <= 1\")\n",
    "                return tensor_\n",
    "            #print(\"dim > 1\")\n",
    "            indices = range(tensor_.size()[1])[::-1]\n",
    "            indices = Variable(torch.LongTensor(indices), requires_grad = False)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                indices = indices.cuda()\n",
    "\n",
    "            return tensor_.index_select(1, indices)\n",
    "\n",
    "        for key in ret:\n",
    "            ret[key] = reverse_tensor(ret[key])\n",
    "\n",
    "        return ret\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGAN_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BGAN_D, self).__init__()\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        self.disc_f = Discriminator()\n",
    "        self.disc_b = Discriminator()\n",
    "\n",
    "    def forward(self, data, mask,args):\n",
    "        score_f = self.disc_f(data, mask, args, 'forward')\n",
    "        \n",
    "        #print(\"=====================================REVERSE===================================================\")\n",
    "        score_b = self.reverse(self.disc_b(data, mask, args, 'backward'))\n",
    "\n",
    "        #print(\"going to merge results\")\n",
    "        score = self.merge_score(score_f, score_b)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def merge_score(self, score_f, score_b):\n",
    "        \n",
    "        #print(\"Foward Scores\",score_f['scores'][0,:])\n",
    "        #print(\"Backward Scores\",score_b['scores'][0,:])\n",
    "        #print(\"Missing\",score_f['missing'][0,:])\n",
    "        #print(\"Foward Scores\",score_f['scores'].size())\n",
    "        #print(\"Backward Scores\",score_b['scores'].size())\n",
    "        #print(\"Missing\",score_f['missing'].size())\n",
    "        \n",
    "        #Calculate Loss for Sigmid layer\n",
    "        Tensor = torch.cuda.FloatTensor\n",
    "        \n",
    "        score_f['scoresSig'] = torch.flatten(score_f['scoresSig'])\n",
    "        score_b['scoresSig'] = torch.flatten(score_b['scoresSig'])\n",
    "        score_f['missing'] = torch.flatten(score_f['missing'])\n",
    "        score_b['missing'] = torch.flatten(score_b['missing'])\n",
    "        \n",
    "        \n",
    "        real_ids = (score_f['missing'].nonzero())\n",
    "        fake_ids = ((1-score_f['missing']).nonzero())\n",
    "        \n",
    "        # Loss function\n",
    "        adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable((score_f['missing'])[real_ids], requires_grad=False)\n",
    "        fake = Variable((score_f['missing'])[fake_ids], requires_grad=False)\n",
    "        validG = Variable((1-score_f['missing'])[fake_ids], requires_grad=False)\n",
    "        \n",
    "        #print(\"Valid\",valid.size())\n",
    "        #print(\"fake\",fake.size())\n",
    "               \n",
    "        #ret_b['scores'] = ret_b['scores'] * ret_b['missing']\n",
    "        #print(\"Final Scores\",ret_b['imputations'][0,:])\n",
    "                \n",
    "        if(fake_ids.size()[0]==0 ):     \n",
    "            loss_gSig=Variable(torch.cuda.FloatTensor([0]), requires_grad=True)\n",
    "        else:\n",
    "            loss_gF = adversarial_loss((score_f['scoresSig'])[fake_ids], validG)\n",
    "            loss_gB = adversarial_loss((score_b['scoresSig'])[fake_ids], validG)\n",
    "            loss_gSig=loss_gF+loss_gB\n",
    "        \n",
    "        loss_dReal = adversarial_loss((score_f['scoresSig'])[real_ids], valid) + adversarial_loss((score_b['scoresSig'])[real_ids], valid)\n",
    "        if(fake_ids.size()[0]==0 ): \n",
    "            loss_dSig = loss_dReal\n",
    "        else:\n",
    "            loss_dFake = adversarial_loss((score_f['scoresSig'])[fake_ids], fake) + adversarial_loss((score_b['scoresSig'])[fake_ids], fake)\n",
    "            loss_dSig = (loss_dReal + loss_dFake)/2\n",
    "        #print(loss_dSig,loss_gSig)\n",
    "        return {'loss_d': loss_dSig , 'loss_g': loss_gSig}\n",
    "\n",
    "    def get_consistency_loss(self, pred_f, pred_b):\n",
    "        loss = torch.pow(pred_f - pred_b, 2.0).mean()\n",
    "        return loss\n",
    "\n",
    "    def reverse(self, ret):\n",
    "        #print(\"in Reverse\")\n",
    "        def reverse_tensor(tensor_):\n",
    "            if tensor_.dim() <= 1:\n",
    "                #print(\"dim <= 1\")\n",
    "                return tensor_\n",
    "            #print(\"dim > 1\")\n",
    "            indices = range(tensor_.size()[1])[::-1]\n",
    "            indices = Variable(torch.LongTensor(indices), requires_grad = False)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                indices = indices.cuda()\n",
    "\n",
    "            return tensor_.index_select(1, indices)\n",
    "\n",
    "        for key in ret:\n",
    "            ret[key] = reverse_tensor(ret[key])\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def run_on_batch(model,discriminator,data,mask,decay,rdecay,args, optimizer,optimizer_d,epoch):\n",
    "        ret_f,ret = model(data, mask, decay,rdecay,args)\n",
    "        \n",
    "        disc = discriminator(ret['originals'], mask, args)\n",
    "        #print(\"BATCH LOSS\",ret['loss'])\n",
    "        #print(\"BATCH LOSS\",disc['loss_g'])\n",
    "        #print(\"BATCH LOSS\",disc['loss_d'])\n",
    "        #print(\"one batch done\")\n",
    "\n",
    "        if optimizer is not None:\n",
    "            #print(\"OPTIMIZE\")\n",
    "            \n",
    "            if (epoch%10==0):\n",
    "                optimizer_d.zero_grad()\n",
    "                disc['loss_d'].backward(retain_graph=True)\n",
    "                optimizer_d.step()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            (ret['loss']+disc['loss_g']).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return ret_f,ret,disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#t = torch.Tensor([1, 0, 3])\n",
    "#ids= (t.nonzero())\n",
    "#ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#t = torch.Tensor([1, 0, 3])\n",
    "#t[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Atena] *",
   "language": "python",
   "name": "conda-env-Atena-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
